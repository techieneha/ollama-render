# Ollama on Render

This repository sets up Ollama (an open-source LLM server) as a public HTTP API using Render.

## Features
- Uses official `ollama/ollama` image
- Runs on port 11434
- Easily integrates with apps deployed on Railway, Vercel, etc.

## Usage
Deploy via Render using this repo:
https://render.com

Then access your model at:
